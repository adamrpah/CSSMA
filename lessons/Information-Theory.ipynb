{"nbformat_minor": 1, "cells": [{"source": ["# What is information theory and why am I wasting your time? \n", "\n", "Information theory is about the quantification, storage, and communication of information. It started in the late 1940s when Claude Shannon prposed it in his paper entitled \"A mathematical theory of communication\".\n", "\n", "So **why** do I bring this up?"], "metadata": {"slideshow": {"slide_type": "slide"}, "collapsed": true}, "cell_type": "markdown"}, {"source": ["**text is inherently about communication**"], "metadata": {"slideshow": {"slide_type": "slide"}}, "cell_type": "markdown"}, {"source": ["# The concept of information theory\n", "\n", "The most basic part of information theory that you all will have experienced is the concept of encoding. If you've ever played an MP3 or opened a zip file you've dealt with this. \n", "\n", "The idea of encoding is to compress information without losing fidelity. As an overly simple example, consider the string:\n", "\n", "`00001111`\n", "\n", "which has 8 characters. If we were to introduce a translation table that said `0` means `0000` and `1` means `1111` then we could reduce that string to:\n", "\n", "`01`\n", "\n", "without losing any information. This is one, extremely basic, aspect of information theory, but it demonstrates the larger principle: how can we quantify the amount of information in some quantity and 'transport' it."], "metadata": {"slideshow": {"slide_type": "slide"}, "collapsed": true}, "cell_type": "markdown"}, {"source": ["# Entropy\n", "\n", "Entropy is one of the foundational calculations in Information Theory and it is effectively similar to the entropy calculation in thermodynamics. In information theory, we use entropy in the form of:\n", "\n", "$H(X)=-\\sum_{i=1}^{n}p_i log(p_i)$\n", "\n", "which is for a discrete variable. The probability of $i$ ($p_i$) depends on the distribution of $X$. Now it is rather easy to understand the earlier formulation given for mutual information, which was\n", "\n", "$I(X;Y)=H(X)-H(X|Y)$\n", "\n", "but could also be written as\n", "\n", "$I(X;Y)=\\sum_{i=1}^n\\sum_{j=1}^m p_{ij} log\\frac{p_{ij}}{p_{i}p_{j}}$\n", "\n", "so the difference in one distributions' entropy minus the amount of entropy that is conditional on a second distribution. "], "metadata": {"slideshow": {"slide_type": "slide"}}, "cell_type": "markdown"}, {"source": ["We can then code a function to calculate entropy given a list of words."], "metadata": {"slideshow": {"slide_type": "slide"}}, "cell_type": "markdown"}, {"execution_count": 3, "outputs": [], "source": ["#Exercise\n"], "metadata": {"slideshow": {"slide_type": "slide"}, "collapsed": true}, "cell_type": "code"}, {"source": ["The value of entropy increases as the system's disorder grows, which means that more unique words are used in the context of text.\n", "\n", "We can also use the entropy implementation in scipy stats, which removes the need to write it ourselves (we do however, have to transform the word list to counts)."], "metadata": {"slideshow": {"slide_type": "slide"}}, "cell_type": "markdown"}, {"execution_count": 5, "outputs": [{"execution_count": 5, "data": {"text/plain": ["1.242453324894"]}, "metadata": {}, "output_type": "execute_result"}], "source": ["import scipy.stats as stats\n", "\n", "def freq_calc(wordlist):\n", "    wordcounts = list( Counter(wordlist).values() )\n", "    return wordcounts\n", "\n", "stats.entropy( freq_calc(['The', 'party', 'party', 'party', 'ran', 'late']) )"], "metadata": {"slideshow": {"slide_type": "slide"}}, "cell_type": "code"}, {"source": ["Compare the entropy of Iago and Othello's speaking parts to that of the lexical divergence"], "metadata": {"slideshow": {"slide_type": "slide"}}, "cell_type": "markdown"}, {"execution_count": 6, "outputs": [], "source": ["#Exercise\n"], "metadata": {"slideshow": {"slide_type": "slide"}, "collapsed": true}, "cell_type": "code"}, {"source": ["And now we can see an issue - Othello has a larger lexical diversity but a smaller entropy. \n", "\n", "Lexical diversity is a rather simplistic measure - and as such it will be more sensitive to mild differences in the number of unique words as the size of the total vocabulary decreases. These differences are not likely to be statistically significant, but they can give the appearance of a larger difference than truly exists. \n", "\n", "In this example, lexical diversity would suggest that the difference in vocabulary complexity is twice as much as with entropy (when we already know that this difference is not significant from our test in the previous notebook)."], "metadata": {"slideshow": {"slide_type": "slide"}, "collapsed": true}, "cell_type": "markdown"}, {"source": ["# Surprise! \n", "\n", "The other concept that I want to talk about is the Kullback-Leibler (KL) divergence. The KL divergence is a measure of relative entropy, so how much entropy there is in dataset $P$ given knowledge of $Q$. Its output is continuous from 0 to 1, and when $D_{KL}=1$ one would never expect to see $P$ after having learned from $Q$. This range of outputs and its meaning is why the KL divergence can be simplified to an explanation of one's 'surprise' at seeing a dataset.\n", "\n", "The KL divergence is implemented as:\n", "\n", "$D_{KL}(P||Q)=-\\sum_i P(i)log\\frac{Q(i)}{P(i)}$\n", "\n", "where each $i$ is a discrete support. \n"], "metadata": {"slideshow": {"slide_type": "slide"}}, "cell_type": "markdown"}, {"source": ["# The value of surprise\n", "\n", "We can do a basic, but not very meaningful, example that compares othello's text given iago's. Implement the KL divergence function and calculate it."], "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"execution_count": 24, "outputs": [], "source": ["#Exercise\n"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"execution_count": 26, "outputs": [{"execution_count": 26, "data": {"text/plain": ["0.27989935534541943"]}, "metadata": {}, "output_type": "execute_result"}], "source": ["kldivergence(iago_support, othello_support)"], "metadata": {}, "cell_type": "code"}, {"source": ["And of course, we could just use the scipy function to calculate this (but it wouldn't be as obvious as to why we have to restrict the input to shared words only)."], "metadata": {}, "cell_type": "markdown"}, {"execution_count": 21, "outputs": [{"execution_count": 21, "data": {"text/plain": ["0.27863256873708608"]}, "metadata": {}, "output_type": "execute_result"}], "source": ["stats.entropy(othello_support, qk = iago_support)"], "metadata": {}, "cell_type": "code"}, {"source": ["# The real value of KL divergence. \n", "\n", "KL divergence has a very straightforward interpretation as surprise and it is bounded from 0 to 1 (making its interpretation intuitive). \n", "\n", "The value in this metric is it provides a way to quantify the amount of change from one document to the next (or one type of document to the next). It provides an easy, grounded way to assess the change in a document over time (think financial reports from quarter to quarter) or between documents that one thinks should be similar."], "metadata": {"collapsed": true}, "cell_type": "markdown"}], "metadata": {"celltoolbar": "Slideshow", "kernelspec": {"language": "python", "name": "Python [Root]", "display_name": "Python [Root]"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.5.1", "pygments_lexer": "ipython3", "file_extension": ".py"}, "anaconda-cloud": {}}, "nbformat": 4}
