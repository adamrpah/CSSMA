{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../Data/www/styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis\n",
    "\n",
    "In this unit we will learn the basics of analyzing structured data. In the process we will cover:\n",
    "\n",
    "* What is structured data\n",
    "* How to use Pandas to read and write structured data\n",
    "* Basic indexing operations of Pandas\n",
    "* Basic operations (math and plotting) with Pandas\n",
    "* Handling dates and times\n",
    "* The 'split-apply-combine' framework for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Pandas\n",
    "\n",
    "Pandas is a contraction for \"panel data\", which is kind of an obtuse saying. A shorthand way of thinking about it is to think of it as dealing with a connected series of data, like a company's stock price over time. Particularly, most spreadsheets fall into this category so pandas is the most natural way of working with Excel-type data using Python. \n",
    "\n",
    "\n",
    "## Pandas is not very pythonic\n",
    "\n",
    "Actually **using** Pandas and not just using it to read files can be conceptually difficult and is a bit of a mental switch compared to most of what we have learned so far. If you want to iterate over things, you can't use a `for` loop easily. Instead you'll need to use specific Pandas methods to do whatever functions you want. These little differences add up and can wear on you, which might make you want to stop using Pandas. That's a fine way to feel (you don't really *have* to use it), but there are some big benefits to using it, that for a lot of people, are worth the costs.\n",
    "\n",
    "## Benefits of Pandas\n",
    "\n",
    "1. Pandas handles a lot of file I/O drudgery for you. I'll show you this in a bit, but reading CSV files and accessing data in them is super simple\n",
    "2. Pandas has a lot of *magic* built into, automaticallly taking care of many type conversions after reading a file\n",
    "3. Using Pandas **is** like working with [SQL](https://en.wikipedia.org/wiki/SQL) (don't know what SQL is? Don't worry, it's a bit advanced for this course but is surely something you'll encounter if you continue to program so it's worth reading up on). So learning Pandas means that you'll have a good idea of the underpinnings how SQL databases work which might help you later in your programming education (although the syntax is different).\n",
    "\n",
    "If you like using or want to continue using Pandas here is some recommended additional reading\n",
    "\n",
    "The Pandas tutorial pages http://pandas.pydata.org/pandas-docs/stable/tutorials.html\n",
    "\n",
    "10 minutes to Pandas http://pandas.pydata.org/pandas-docs/stable/10min.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We start with importing the packages\n",
    "import pandas as pd #pandas is almost always imported as pd. Just because. \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "#We turn off the latex usage in matplotlib because LaTeX doesn't know\n",
    "#how to handle a '_' character without it being escaped with a backslash\n",
    "#Since we use '_' in column names typically this can be a bit of a problem\n",
    "#If we don't turn this off\n",
    "mpl.rc('text', usetex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is an IPython \"magic\" to make sure that plots appear\n",
    "#directly in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structures in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('../../data/majors/recent_Arts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Pandas reads in a `CSV` it turns it into its own data structure called a `Dataframe`. This `Dataframe` is actually a Python class, you can think of it as just a type of *object*. Our data is inside this *object* and it controls how we can interact with it (so you can see the first difference between this and regular programming).\n",
    "\n",
    "\n",
    "(The nice formatting that makes it look like an Excel spreadsheet is provided by IPython Notebook!)\n",
    "\n",
    "Now, let's actually load this `CSV` into a variable so we can explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/majors/recent_Arts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basics of a dataframe\n",
    "\n",
    "As usual, the choice of variable names is whatever you so choose. However, since the object we're working with is a `Dataframe` you'll very frequently see people assign this to a variable called df. \n",
    "\n",
    "A `Dataframe` has two basic ways to access values inside of it.\n",
    "\n",
    "The **columns** run across the **top**\n",
    "\n",
    "The **indices** run down the **left** (for now, you can think of these as rows)\n",
    "\n",
    "We can get see the variables by calling them by name from the `dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The columns are the labels across the top\n",
    "print( df.columns ) \n",
    "print()\n",
    "\n",
    "#The indexes run down the side\n",
    "print( df.index )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Data in Pandas\n",
    "\n",
    "Pandas supports two methodologies to access data that is stored in a column. The first is  just by typing the name of the column after the variable name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can access a column with the . notation shown here\n",
    "df.Major_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other way is similar to accessing the values of a key in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or we can access a column of data like we access the value of a key in a dictionary\n",
    "df['Major_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the indices, we can slice just like it was a list and get rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can slice like in a list, and we'll get those columns \n",
    "#Notice how the row indexes are numeric and that's what we slice on\n",
    "df[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we have these ways of accessing:\n",
    "\n",
    "* Accessing like a dictionary is a column\n",
    "* Accessing like a list is the index (row)\n",
    "\n",
    "and accessing a column name after a dot is something that we can do because the data is wrapped in the `Dataframe` class but in practice is just a shorthand way of accessing the column as if it were a dictionary.\n",
    "\n",
    "However, these direct methods of accessing can be a bit limiting. For this reason Pandas has three additional methods that allow us to *slice* a dataframe and return specific rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing in Pandas\n",
    "\n",
    "Pandas has 2 methods besides to direct access to slice/index data\n",
    "\n",
    "* .iloc is integer based and works on the index\n",
    "* .loc is strictly label based\n",
    "\n",
    "To distinguish these concepts I'm going to sort the matrix so that the index labels are **not** in the same order as their position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can sort by a single column\n",
    "sdf = df.sort_values('Total')\n",
    "sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use the iloc method on sdf we get the first row in the new sorted `dataframe`\n",
    "Check it out in comparison to above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works the same as if we were indexing the `dataframe`, but the difference is that when we index, we can't access just a single row. It requires that we give a range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iloc method can handle a slice and it will return the same number of rows, in the same positions just like when we slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.iloc[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the **big** difference between direct slicing on the dataframe and using the iloc method is that we can actually slice **both columns and rows**.\n",
    "\n",
    "When we access both columns and rows the syntax is:\n",
    "\n",
    "`dataframe.iloc[row_slice, column_slice]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.iloc[0:3, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want **all** of the rows but only some of the columns we can do that too. We just need to give it an empty slice in the row column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.iloc[:, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is the `iloc` method, it allows us to slice a dataframe using integer-based labels given the order of the dataframe.\n",
    "\n",
    "So now let's move on and see the differences that occur when we access rows by their index label using the `loc` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.loc[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use the loc method, it looks for the row labeled '6'. Notice how this was the third line in the `dataframe` (as printed above in order). So the `loc` method retrieves rows based on thier index label, which does not need to be their position in the `dataframe`!\n",
    "\n",
    "This allows us to also access columns using just their names which is really convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also use the loc method with an index label and column labels\n",
    "#Note how we can slice on columns also! But we can only slice across labels\n",
    "sdf.loc[2, 'Major_code':'Major_category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access all the rows, but only some of the columns similarly as with `loc`, just give a `:` for the rows and then the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.loc[:, 'Major_code':'Major_category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Series: Pandas' other data type (that you'll use)\n",
    "\n",
    "You may have noticed that when we selected only one row or column, it printed differently than when we print the `dataframe`. That's because a dataframe is for 2-dimensional data (meaning that it has multiple rows and columns). When we have 1-dimensional data that is a Pandas `series`.\n",
    "\n",
    "The same methods work on a `series` as a `dataframe`, the only difference is that it only has one column which will be without a label. I'll demonstrate by creating and indexing a `series` right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tseries = sdf.loc[0]\n",
    "tseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A series is indexed just like a list\n",
    "tseries[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#But it can also be indexed by its row labels\n",
    "tseries['Major_code':'Major_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the iloc method, but now it returns a single value\n",
    "tseries.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or the loc method, which also returns a single value\n",
    "tseries.loc['Major_category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's go over the benefits of using Pandas\n",
    "Okay, some of that might seem tedious, but it's really important that you get a grasp of the fundamental ways of working with and manipulating `dataframe` and `series` objects. A lack of understanding of your datatypes is a sure fire way to make mistakes, and if that happens the benefits of Pandas can disappear. And you'll really like some of these benefits...\n",
    "\n",
    "The first one is a lifesaver. Pandas can read (and write) .xls/.xlsx files! Now you don't need to open a workbook in Excel and save it to a CSV everytime someone sends one to you!\n",
    "\n",
    "When we read an Excel spreadsheet, all we have to say is what sheet we want to use in the file. You can use either the sheetname (if it has one) or just give it the index of the sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_df = pd.read_excel('../../data/majors/recent_Arts.xlsx')\n",
    "excel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's not just excel. Pandas can actually read/write a large number of different and really useful file formats that can be essential when working with collaborators who might not be quite so python inclined. This is the full list:\n",
    "\n",
    "        read_csv\n",
    "        read_excel\n",
    "        read_hdf\n",
    "        read_sql\n",
    "        read_json\n",
    "        read_msgpack (experimental)\n",
    "        read_html\n",
    "        read_gbq (experimental)\n",
    "        read_stata\n",
    "        read_clipboard\n",
    "        read_pickle\n",
    "\n",
    "But really the only two you'll probably ever use are Excel and CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Pandas helps you quickly explore and manipulate data as you learn about your dataset basics. One quick benefit is the built-in plotting directly from the dataframe.\n",
    "\n",
    "Let's say that we wanted to make a plot that examined the difference between the majors in terms of the raw employment numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Employed'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so it's pretty ugly, but it was also super quick!\n",
    "\n",
    "Let's actually think about this for a moment because something magical just happened and we all probably took it for granted.\n",
    "\n",
    "### We just plotted numeric data from the file that I read in with a single command.\n",
    "\n",
    "### When did I change the type of that data to be an integer so that we could plot it??\n",
    "You might recall when we read files in the past using `open('super_cool_file.csv')`, everything was read in as a string by default, even numbers!\n",
    "\n",
    "When we load data with Pandas it automatically converted the 'Employed' column data to integers. In fact, Pandas does this with all of the columns and when it does this it picks the **least** expansive data type that **accommodates all the data in the column**.\n",
    "\n",
    "We can check this, so the `Unemployed` column should be integers also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Unemployed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that means that the `ShareWomen` column should be floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ShareWomen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something to be aware of though, is that if we had a single string in that column of data **none of it would be converted**. All of the read values would be strings because any number can represented as a string, just as text data can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should really change the plot type so that it displays the data as a bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Employed'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah! That's a little bit better!\n",
    "\n",
    "But we should never have a graph without a y-label! To change labels we'll need to operate on the `matplotlib` graph. Whenever we call a `plot()` off of a dataframe it always retuns a Matplotlib axis object that is our graph. We can modify that graph to add labels and other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm changing the color of the bars here because the other blue looks awful!\n",
    "ax = df['Employed'].plot(kind = 'bar', color = 'steelblue')\n",
    "#Now I can set the y-axis label\n",
    "ax.set_ylabel('Students Employed')\n",
    "#I can also set the xticks to the major names\n",
    "major_labels = df['Major']\n",
    "#Here I set the xtick labels. The `;` suppresses matplotlib print statements\n",
    "ax.set_xticklabels(major_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have the matplotlib axis object, we can do anything with it that would normally with a `matplotlib` graph.\n",
    "\n",
    "## But wait, there's more!\n",
    "\n",
    "Pandas has mathematical functions built directly into the `dataframe`. So if we want to know the average of a column or the number of rows with entries (not every position has to have a value!) we can do that quickly and easily.\n",
    "\n",
    "Let's start by just counting the number of values in each column (it should be 8 in every column since every spot in our spreadsheet was filled out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But I also want to show you what happens when we have a column with no values entered for it. When there is a missing value in the raw data Pandas replaces that value with a `Not a Number` or `NaN` value from numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['Test_column'] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we count, it won't be the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also just count a few columns or single column by chaining the `count()` function after we index the Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Employed'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0:2].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other useful functions built in too. We can quickly take the mean or median of a column also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Employed.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Employed.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can even get the mean, medians, and a host of other summary statistics for all columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also quickly do row/column operations in Pandas\n",
    "\n",
    "What we're typically interested in is the percentage of Employed people out of the Total number of people for a given major. We can do that easily with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can divide an entire column by another column\n",
    "df['Employed']/df['Total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What it does is divide each value in the `Employed` column by the value in the `Total` column that is in the same row. It does that for all of the rows simultaneously with just one line of code!\n",
    "\n",
    "If we want to save this column of data (which we will) we can just make up a new name and assign the output to our `dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And now use this to add a new column to the dataframe\n",
    "df['Percent_Employed'] = df['Employed']/df['Total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check that it worked below. I'm going to use the `head()` function to print the `dataframe`. `head()` will only print the number of rows given as the argument. It helps save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We can perform complicated functions to calculate new columns too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm only doing this for show (this calculation doesn't make any sense)\n",
    "(df['College_jobs'] - df['Non_college_jobs']) ** 2 / 700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same with rows, but it's very unlikely that we will have a situation like this that makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can perform row level operations, making a new row\n",
    "#Notice how each column is represented?\n",
    "df.loc[0, 'Employed']/df.loc[1, 'Employed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying - or the database mentality\n",
    "\n",
    "So far we've just directly accessed rows and columns in the `dataframe`, but there's actually another way to get data. We can do this by *querying* the data. We can make comparions with greater than, less than, or equal signs to get only the rows or columns of the dataframe that meet our criteria.\n",
    "\n",
    "So now let's only consider majors that have more than 20,000 students that graduated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Total > 20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that only the 5 rows that had more than 20,000 students were selected.\n",
    "\n",
    "But how does this really work?\n",
    "\n",
    "The greater than symbol doesn't need to be used just inside the `[]` and it isn't **actually** instructing the dataframe what to do directly. When we use an expression with the dataframe we are actually creating a **mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Total > 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's going on is that on the column of interest it evaluates if the statement is `True` or `False` and then pipes that truth array into the dataframe. Then any rows that were `True` are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Total > 20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat, huh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can chain selection operators off of the query also if we want to know a bit more about the resulting column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Employed > 20000].Unemployment_rate.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use more than one criterion to select rows in our `dataframe`. The syntax for this is very \"un-pythonic\", but it's a very useful way to get just the part of the data that you are interested in. To do that we have to use this syntax:\n",
    "\n",
    "`df[(first expression) & (second expression) | (third expression) ...]`\n",
    "\n",
    "Each expression has to be enclosed in parentheses.\n",
    "\n",
    "Expressions can be chained with an `and` statement, which must be represented as `&`.\n",
    "\n",
    "Alternatively you can chain expressions with an `or` statement, which must be represented as `|`.\n",
    "\n",
    "You can have as many statements as you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.Total > 20000) & (df.ShareWomen > 0.50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just for fun I'll demonstrate the same statement with the `|` operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.Total > 20000) | (df.ShareWomen > 0.50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can see here that we now have 7 of the 8 majors printing. Only one major (Miscellaneous fine arts) does not have either more than 20,000 students that have graduated recently or a Share of Women in the major greater than 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that you've seen the biggest conceptual shift between working with Pandas and analyzing data with raw Python.\n",
    "\n",
    "When you directly program code to work with data, you tell it what to do, step by step. We would go through each value and see if it met our conditions like this:\n",
    "\n",
    "    for total_students in total:\n",
    "        if total_students > 20000:\n",
    "            #Continue with code\n",
    "            \n",
    "We're basically writing a procedure for how things should be done. It's very readable and, if we keep our statements separated, it's very easy to debug.\n",
    "\n",
    "Pandas doesn't work like that. You don't tell it **what to do** really, you tell it **what you want** and it figures out how to get it to you. \n",
    "\n",
    "For someone that's been doing data analysis without Pandas or heavily relying on SQL this is an unsettling shift. That unsettling feeling isn't unwarranted, it's a tough concept to get used to and 'hiding' the mechanics of what is going on can easily lead to errors (especially when you try to execute a complicated statement right away). \n",
    "\n",
    "The worst part is that in a lot of these cases, Pandas will dutifully execute the statement you asked for and not provide an error (since technically there wasn't an error, just a gap in translating what you wanted/how you wanted the data handled to the machine). \n",
    "\n",
    "My advice though is to start slowly. IPython Notebook changes everything when it comes to learning complicated packages, making it possible to both learn a package and, whenever you start a new project, your data.\n",
    "\n",
    "Even with these hiccups, which is more related to this style of analysis than the library, Pandas has some amazing features that make it really worth using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dates - the magical transformation\n",
    "\n",
    "We talked earlier about how Pandas automagically converts data types when reading a file in. One of the best automagic features is Pandas' ability to handle time. To show you how amazing this is, I'll also teach a bit about how we would handle time without using Pandas.\n",
    "\n",
    "Typically we would use the `datetime` library to handle time in regular code. `datetime` is a real workhorse of a library (and you'll still need to use it), but it can be a bit unfriendly.\n",
    "\n",
    "One unfriendly bit is that the module we use most has the exact same name as the package itself. Make sure to pay attention to code online to see if they are importing `datetime` the package or `datetime` the module!\n",
    "\n",
    "We're going to import the package so that we can access all of the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "datetime.date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want just the `date` we can use the `date` module in datetime.\n",
    "\n",
    "But typically most people care about what time it is too. To get both the date and the time, we must use the `datetime` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's how dates and times are generated! The benefit is that the time is erturned in a `datetime` object so we can access individual parts of the time by name, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.today()\n",
    "print(date.year)\n",
    "print(date.month)\n",
    "print(date.second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we read in a list of dates? For that we actually have to convert a string using the `datetime.datetime.strptime` function. This function takes two arguments - \n",
    "    1. the date string itself\n",
    "    2. a string telling it the format of the date string using a special [symbol set](https://docs.python.org/2/library/datetime.html)\n",
    "    \n",
    "So for every string, you have to write the format that the time is in (exactly!) in order for `datetime` to convert it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime.strptime('2014-01-01', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But do you think that all dates in all text files are formatted the same?\n",
    "\n",
    "Absolutely not! Every file and every program typically writes out dates and times in a different format. Which would you require to write some really complex code to automatically read a file and convert those dates to a format that you can use (it's for this reason that the Python datetime web page used to be one of my most visited!).\n",
    "\n",
    "But Pandas can take care of all of that for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl = pd.read_csv('../../homeworks/data/aapl_stock_price.csv')\n",
    "aapl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.head().Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It automatically read in the dates! It understands time and can use basic functions with the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.Date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.Date.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and can sort the dataframe based on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aapl.Date.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily select a certain period of time using queries! To specify a date, we don't even have to create a datetime object - we can just use a string representation of the time and Pandas will translate it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl[aapl.Date > '2015-08-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl[(aapl.Date > '2015-08-01') & (aapl.Date < '2015-08-18')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these dates can work without specifying the whole date too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl[aapl.Date > '2015']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, by default we can't access individual attributes of a date value this way (because the value, despite being worked with as a date isn't actually a datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.Date[0].month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See, Pandas is actually doing all of this with the date as a string!  If we need real datetimes though, Pandas has a function to convert it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl['dtDate'] = pd.to_datetime(aapl.Date)\n",
    "aapl.dtDate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.dtDate[0].month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And even better, Pandas has the ability to plot timeseries with the dates directly! You just have to set the datetime column as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.set_index('dtDate', inplace=True)\n",
    "ax = aapl.Close.plot(color='steelblue')\n",
    "ax.set_ylabel('Closing Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced built-in functions\n",
    "\n",
    "Pandas also has a number of built-in functions that are a bit more advanced and useful with structured analysis. One great function is the `rolling_mean()`, which calculates a mean given a certain size window (remember when we had to write a function to do that in the [Sentiment Analyis lesson](Day5_pm2_Sentiment-Analysis.ipynb) )?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.Close.rolling(260).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are roughly 260 workdays in a year\n",
    "close_moving_avg = aapl.Close.rolling(260).mean()\n",
    "#Now we just plot it\n",
    "ax = aapl.Close.plot(color='steelblue')\n",
    "close_moving_avg.plot(label='Moving Average', color='green')\n",
    "ax.set_ylabel('Closing Price')\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we were interested in the movement of the stock between each day (i.e. did the stock lose or gain money from one day to another)?  We can do that easily with the `diff()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.Close.diff().plot(color='steelblue')\n",
    "ax.set_ylabel('Daily Return')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And remember at anytime we can restrict the dataset with a query and plot only that portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl[aapl.Date < '2014'].Close.diff().plot(color='steelblue')\n",
    "ax.set_ylabel('Daily Return')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you're seeing above is lots of function calls one on top of the other. This is called `chaining` (and isn't very pythonic), but can be very useful. We can do this with most functions, watch as we finish with the mean or standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl[aapl.Date < '2014'].Close.diff().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl[aapl.Date < '2014'].Close.diff().std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl[aapl.Date < '2014'].Close.diff().median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl[aapl.Date < '2014'].Close.diff().hist(bins=100, color='steelblue', log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split-Apply-Combine\n",
    "\n",
    "Pandas also has a codified pipline to handle faceting data, they refer to it as [Split-Apply-Combine](http://pandas.pydata.org/pandas-docs/stable/groupby.html). In laymans terms, this means:\n",
    "\n",
    "1. Split the data into groups\n",
    "2. Compute some function or transform the data in some way\n",
    "3. Combine into a new dataframe that has the groups as rows\n",
    "\n",
    "To test this we will work with the Major data again, we will probably be interested in comparing the different major categories. How many major categories do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woops! To do a meaningfull analysis, we'll need to read in the rest of the files for the other majors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ../../data/majors/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in all of the `recent_*csv` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But a list or dictionary of separate dataframes defeats the real utility of Pandas! We could go through the list and keeping adding all of the dataframes together, but instead we can use the built-in `concat()` function to put a list of dataframes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf = pd.concat(majordfs)\n",
    "fulldf.Major_category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One quantity that we might be interested in is the percentage of graduates who are in low wage jobs per major category. To do that, the first thing we would have to do is group all of the majors by their major category. To do that, we use the `groupby()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = fulldf.groupby('Major_category')\n",
    "dir(grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grouped dataframe is a bit different than the regular dataframe, it's because it's storing all of the data grouped together.\n",
    "\n",
    "There are analytical functions that we can run on the entire grouped dataframe, such as getting the variance or mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use any of these functions on an individual column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.Men.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can't use a function on the individual columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.Low_wage_jobs / grouped.Employed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So any column that you want to use in the grouped analysis should be generated before you group the dataframe. Otherwise, you'll need to create the column in each group dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf['Pct_Low_wage'] = fulldf.Low_wage_jobs/ fulldf.Employed\n",
    "grouped = fulldf.groupby('Major_category')\n",
    "grouped.Pct_Low_wage.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But typically we will want to know more about a grouped column of data than just the mean (or I really hope you do). Other quantities you should care about are median, standard deviation, and standard error. \n",
    "\n",
    "That is where the aggregate function comes in handy (this is where the `Apply` and `Combine` come into play). The `agg` function allows you to specify multiple functions at the same time and returns a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.Pct_Low_wage.agg(['mean', 'median', 'std', 'sem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we could even combine this and plot it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = grouped.Pct_Low_wage.agg(['mean', 'median', 'std', 'sem'])\\\n",
    "                         .loc[:, ['mean', 'sem']]\\\n",
    "                         .plot(kind='bar', yerr = 'sem', color='steelblue')\n",
    "ax.set_ylabel('Percentage Low Wage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use functions that are outside of Pandas too and apply them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.Pct_Low_wage.agg([np.sum])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or write our own functions if we give it as a dictionary and use a `lambda` function (`lambda` functions create a function in a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "grouped.Pct_Low_wage.agg({'NoisyAverage': lambda x: np.mean(x*random.random())})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Getting your data out of Pandas\n",
    "\n",
    "Sometimes you might really want to work with the data in a Pandas `dataframe` using regular Python code. At any time you can extract the data in a row, column or the entire dataframe.  We can do that simpy by appending `.values`.\n",
    "\n",
    "When we ask for the `values` from a Pandas `dataframe` it returns our old friend, the Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Total.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There, now we have an array of the actual raw values. We can iterate through these easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in df.Total.values:\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to have a list instead of a Numpy array (because they aren't the same thing!) we can do that easily too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Total.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had any of the manipulations we performed here actually been important, we'd of course want to save them. No more writing each item in the boring old python way, Pandas has a way to do this all in one line. All of the file formats that we could read, we can also write to. For a full list just type df.to follwed by TAB. For now, here is our excel example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('../../data/majors/recent_Arts_edited.xlsx', sheet_name='Sheet1', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Exercises\n",
    "\n",
    "Read in the data file for recent graduates in business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf = pd.read_csv('../../data/majors/recent_Business.csv')\n",
    "bdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the average unemployment rate greater for business or arts students?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_unemployment_rate = bdf.Unemployment_rate.mean()\n",
    "art_unemployment_rate = df.Unemployment_rate.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we are only interested in the majors that have more than 20,000 students for each? Which category has the lowest unemployment rate then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which major category has the greater share of women students when we consider majors with >20,000 students?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises completed!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
