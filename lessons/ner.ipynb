{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def css_styling():\n",
    "    styles = open(\"../data/www/styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "You got a taste of named entity recognition (NER) at the very end of our first introduction to text processing and analytics. NER is relatively simple in concept, what we want models to do are to recognize that \n",
    "\n",
    "`Harriet Tubman`\n",
    "\n",
    "is not just the written words `Harriet` and `Tubman`, but that `Harriet Tubman` is the `name` of a `person`. This requires more semantic understanding of what is being written.\n",
    "\n",
    "NER is one of the core tasks and challenges in NLP. Machine learning models are at the heart of the ability of modern packages to perform out of the box NER at a high-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp('Apple had a strong third quarter, with revenues of more than 3 trillion USD reported.')\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy performs entity recognition out of the box with a loaded model like `en_core_web_lg`. In the sample document it identifies three entities `Apple`, `a strong third quarter`, and `more than 3 trillion USD`. We can investigate further what entities it thinks it has found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent)\n",
    "    print(ent.label_)\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these entities have triggered the trained labels in `en_core_web_lg` of `ORG`, `DATE`, and `MONEY`. We can also see this in-line in a nicer way too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entites have to be taught to a model, you can think of the set of entity labels that are used as a sort of 'ontology' that the model will be taught about how to view the world. You can check on what entities are built in by looking at the source documentation for the [model itself](https://spacy.io/models/en#en_core_web_lg).\n",
    "\n",
    "Out of the box `en_core_web_lg` is taught to recognize `CARDINAL, DATE`, `EVENT`, `FAC`, `GPE`, `LANGUAGE`, `LAW`, `LOC`, `MONEY`, `NORP`, `ORDINAL`, `ORG`, `PERCENT`, `PERSON`, `PRODUCT`, `QUANTITY`, `TIME`, and `WORK_OF_ART`. You can also check out the context of the dataset that it was trained in -- namely web documents.\n",
    "\n",
    "Proper training is important, because it helps increase the flexibility of the model to less than perfect written context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('apple had a strong third quarter, with revenues of more than 3 trillion USD reported.')\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This year's apple phone is the worst version yet.\")\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that does not mean that it is impervious to written language. General purpose NER models have limits on their ability to recognize all entities -- it is essential that you profile the model performance on a test corpus that you have evaluated **by hand** so that you become familiar with where the model experiences failure and how it performs generally.\n",
    "\n",
    "# Context and localization\n",
    "\n",
    "In that vein, it is important to assess model performance when you change contexts.\n",
    "\n",
    "Let's examine Othello again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "othello = open('../data/Othello.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can identify and clean up the lines to run it through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "othello[86:260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = ' '.join(othello[86:260].split('RODERIGO. ')[-1].split(' IAGO.')[0].split())\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(line)\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "othello[694:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(othello[700:1000])\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The truth is that the formatting has little effect to the NER model (talking about line breaks -- not the existence of whitespace at all, whih is crucial) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(' '.join(othello[700:1000].split()))\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But identification can be sensitive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(' '.join(othello[695:1000].split()))\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would I bring up an example that chops a word in half? You may think of always having \"perfect\" replication of your source material as data by default, but there is a wide range of data that you may be interested that comes through an OCR pipeline (i.e. scanned physical documents). The context of your data is related both to its method of creation into its current form and the context of its data (subject, time of creation, culture/language written in). These are the important details that you need to take into account when using a pre-trained NLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(othello[:1009])\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can tell that there is a context issue -- that the `en_core_web_lg` model has not been trained with the text of plays. \n",
    "\n",
    "# Custom trained models\n",
    "\n",
    "The advantage of progess in the NLP space has been the sharing of pre-trained models. There are a number of pre-trained models and surrounding packages that are appropriate for different contexts. \n",
    "\n",
    "Here we will try [bookNLP](https://github.com/booknlp/booknlp), which has been trained for books. bookNLP uses spaCy for POS tagging and bert models for its other functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install booknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Housekeeping for multiple openMP start-ups\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from booknlp.booknlp import BookNLP\n",
    "\n",
    "model_params={\n",
    "        \"pipeline\":\"entity,quote,supersense,event,coref\", #This is not the full pipeline for speed\n",
    "        \"model\":\"small\" #This is for our laptops\n",
    "}\n",
    "\n",
    "booknlp=BookNLP(\"en\", model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=\"../data/Othello.txt\"\n",
    "\n",
    "output_directory=\"output_dir/othello/\"\n",
    "\n",
    "book_id=\"othello\"\n",
    "\n",
    "booknlp.process(input_file, output_directory, book_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls output_dir/othello/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots to unpack here. The first thing we'll want to look at is the `book.html` -- this will give us the final summary and the annotated text of the passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!open output_dir/othello/othello.book.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine all of the other files too. Each one of these is structured as a table for further use. Tokens has every token, entities has the list of found entities, quotes are quotes form the text, and supersense is tagging related to wordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output_dir/othello/othello.entities', sep='\\t')\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On your own\n",
    "\n",
    "Evaluate the outputs and gather a sense of where the package struggles with Othello -- what is working and what is not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom training a NER model\n",
    "\n",
    "There will come a point where you will need to train your own NER model -- either it differs too much from the context for available models, you have a need to improve performance, or you need custom entity classes. When you train a NER model you will generally select a pre-existing, trained NER model and give it training data with your labelled entities. \n",
    "\n",
    "Selecting a pre-existing model will have many influences -- what is the intended purpose? is there a subject specific training model? what hardware do you have available to use or can afford to use?\n",
    "\n",
    "For our purposes in class we will use spaCy for simplicity. Just know that using transformer models with huggingface is effectively the same. \n",
    "\n",
    "## Step 1 -- Training Data\n",
    "\n",
    "To keep things simple we will introduce a new `SPEAKER` entity, which will recognize and identify instances where a `SPEAKER` occurs. Creating training data is straightforward but tedious -- we need to build a set of docs, with the entity of interest and its character spans identified like so:\n",
    "\n",
    "```\n",
    "training_data = [\n",
    "  (\"Tokyo Tower is 333m tall.\", [(0, 11, \"BUILDING\")]),\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First I want to make a directory to hold all of our data that we will be generating\n",
    "os.mkdir('corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise.\n",
    "\n",
    "Create 1,000 training data examples of `SPEAKER` for us to use in our pipeline as a `training_data` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training data, we will split it and we will process it for the spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "db = DocBin()\n",
    "for text, annotations in training_data[:400]:\n",
    "    doc = nlp(text)\n",
    "    try:\n",
    "        ents = []\n",
    "        for start, end, label in annotations:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    except ValueError:\n",
    "        print( text, annotations)\n",
    "db.to_disk(\"./corpus/train.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And now for the evaluation data\n",
    "db = DocBin()\n",
    "for text, annotations in training_data[400:800]:\n",
    "    doc = nlp(text)\n",
    "    try:\n",
    "        ents = []\n",
    "        for start, end, label in annotations:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    except ValueError:\n",
    "        print( text, annotations)\n",
    "db.to_disk(\"./corpus/dev.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're essentially just working on the command line (but we will do it from the notebook). We're going to need a configuration file -- spaCy has nice interactive web site to do so: https://spacy.io/usage/training\n",
    "\n",
    "Get the configuration and save it as `corpus/base_config.cfg`\n",
    "\n",
    "Once we have our base configuration then we can fill it in for our specific machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init fill-config ./corpus/base_config.cfg ./corpus/config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train ./corpus/config.cfg --output ./corpus/ --paths.train ./corpus/train.spacy --paths.dev ./corpus/dev.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as we can see, we achieve a good model extremely quickly. From the outputs we can load up our trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_nlp = spacy.load('corpus/model-best/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = sp_nlp(training_data[0][0])\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And go a bit bigger on the text size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = sp_nlp(othello[:1209])\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = sp_nlp(othello[4000:5500])\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disambiguation\n",
    "\n",
    "Disambiguation is fundamentally different from the NER task that we have been dealing with and follows as a next step once you have extracted entities. \n",
    "\n",
    "Disambiguation is **extremely difficult** in almost any real big data setting. There is just no way around the fact that text data that you obtain will contain ambiguity that is impossible to resolve with 100% certainty unless you were able to speak to the data creator or were a part of the data creation process. \n",
    "\n",
    "As a simple example, one of the most active areas of disambiguation research is in Author Name Disambiguation (AND) for scientific publications. If you have two manuscripts that both have the author `Yang, Y.`, how you can be certain that `Yang, Y.` of `Manuscript_1` is the same `Yang, Y.` of `Manuscript 2`? More than one person can have the same name, there can be both a `Paris, TX` and `Paris, FR`, and so on.\n",
    "\n",
    "There is 'duplication' in language, this is why at national levels people have some form of a number (social security number in the US) that is unique to each individual -- even if they have the same name. For locations, each city in the US has a FIPS code (like we used with the census) that is unique, even if a town or city shares a name with a town in another state. \n",
    "\n",
    "Disambiguation is the process of creating these unique entity indices for our own entities -- we are essentially taking all of the entities that we find and trying to reduce the pool down to the unique entities and assign a \"SSN\" to each unique pool of entities. Thus, we know that `IAGO` in line 10 is the same entity as `IAGO` in line 1000.\n",
    "\n",
    "## Exercise Discussion. \n",
    "\n",
    "What features would you think to use in something like author name disambiguation?\n",
    "\n",
    "\n",
    "## Back to our usage\n",
    "\n",
    "We don't have much of a real example to work through here because what we would care about is direct matching. In the real world we would typically have to deal with misspellings or variations in spellings, which is what grows the pipeline. One of the most fundamental techniques in doing disambiguation is leveraging fuzzy matching to generate potential candidates. Names with a high-similarity are considered as potential match candidates for further consideration and low-similarity candidates are discarded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "fuzz.ratio(\"IAGO\", \"IAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio(\"OTHELLO\", \"IAGO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio(\"DESDEMONA\", \"IAGO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio(\"DESDEMONA\", \"OTHELLO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here a dropped character is still giving us a high match ratio, while different names are giving poor matches. Of course choosing where 'high' and 'low' is is effectively another algorithm that you are creating. As such, it needs to be \"trained\" and optimized to be specific to your data. Importantly, fuzzymatching is not a silver bullet since it's relying on numerical measures -- the length of your target strings will have an impact on the calculated ratios (and thus how you might want to set your cut-off for a high match or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
