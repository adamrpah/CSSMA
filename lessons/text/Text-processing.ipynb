{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Processing text\n",
    "\n",
    "The promise of text analysis, to me, appears to be the greatest generator of interest from social scientists into computational, quantitative research. \n",
    "\n",
    "The quantitative analysis of text can return some impressive findings/analytics that aid research questions---however, the difficulty curve rises rapidly as you look to further refine and include detail in your model (as opposed to other areas in Computational Social Science)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The basics of text\n",
    "\n",
    "To start I want to show you the basics of working with text **largely without any packages**. Why? Because you should walk before you run and before you use *automagic* functions you should gain intuition about the text itself. \n",
    "\n",
    "To start we will work with a document that most of you will have some experience with, *Othello* (`../data/Othello.txt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!head -n13 ../data/Othello.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We shoul always start with a look at the file to gain a sesne of what is going on.\n",
    "\n",
    "Within the first 13 lines we already have an example of organizational text, scene directions, and dialogue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we open the entire file and split on `SCENE` we should be able to get a quick sense of how the scene ending/begins transition is handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "othello_full = open('../data/Othello.txt').read()\n",
    "split_scene = othello_full.split('SCENE')\n",
    "#Print through the scenes\n",
    "for i in [2, 3]: \n",
    "    print(split_scene[i][:100])\n",
    "    print('------')\n",
    "    print(split_scene[i][-100:])\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice a pattern? Can you think of a way to clean up non-dialogue text to make our job of extracting dialogue easier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Exercise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our goal is to now separate out the dialogue for each individual character.\n",
    "\n",
    "Create a dictionary with the character name as the key and all of the characters dialogue as a list of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From this point there are many roads that we can take and this data will serve as our foundation. We will not modify the `char_dialogue` dictionary directly to maintain flexibility for different analytical approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bag of words\n",
    "\n",
    "A powerful, if surprising, analytical approach is one where essentially all structure from the text is disregarded. This approach is called appropriately called the bag of words and it can be extremely useful when we have a sufficient volume of text to analyze at an aggregate level.\n",
    "\n",
    "As a first step, we should clean all of the lines into indiviudal words (removing punctuation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At the most basic level, we can quickly get a sense of how much each character speaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted(char_words, key=lambda k: len(char_words[k]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And visualize the distribution of word frequencies. Her we will plot this as a ranked plot (so rank vs. frequency) for each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Zipf's law\n",
    "\n",
    "Zipf's law is an empirical one, that was discovered by the linguist George Kingsley Zipf. \n",
    "\n",
    "This law states that given a large sample of words used, the frequency of any word is inversely proportional to its rank in the frequency table. So the most frequent word will occur about twice as often as the second most frequent word, three times as often as the third most frequent, and so on and so forth.\n",
    "\n",
    "Visually this pattern will emerge as a fat-tailed distribution (possibly a power-law). This law holds for many languages and even smaller corpuses (as opposed to the whole of an entire language). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparing dialogue\n",
    "\n",
    "We can easily start to dig into whether the number of words spoken would really designate one character as being the 'main' character in a play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IAGO.', len(char_words['IAGO.']))\n",
    "print('OTHELLO.', len(char_words['OTHELLO.']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Basic, but we know that there are issues with this when we consider language. Basic problems emerge if we look at the most used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(Counter(char_words['IAGO.']).items(), key=operator.itemgetter(1), reverse = True)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common words and prepositions don't really count for much/encode much information from a quantitative perspective. It is necessary to construct a sentence that is readable to a human, but not necessary to quantify/characterize a text. \n",
    "\n",
    "We could do the work to build a dictionary of words we don't care about, but this has already been done for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwordsfrom nltk.corpus import stopwords\n",
    "stopwords.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can clean out stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def cleaner(wordlist):\n",
    "    temp = []\n",
    "    for word in wordlist:\n",
    "        if word not in stopWords:\n",
    "            temp.append(word)\n",
    "    return temp\n",
    "\n",
    "char_nonstop = {}\n",
    "for char in char_words:\n",
    "    char_nonstop[char] = cleaner(char_words[char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print('IAGO.', len(char_nonstop['IAGO.']))\n",
    "print('IAGO. set', len(set(char_nonstop['IAGO.'])))\n",
    "\n",
    "\n",
    "print('OTHELLO.', len(char_nonstop['OTHELLO.']))\n",
    "print('OTHELLO. set', len(set(char_nonstop['OTHELLO.'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This still isn't perfect though. If we really examine Iago's wordlist, we can see this with the punctuation included in some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_nonstop['IAGO.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "NLTK has a built in word tokenizer to help with these situations. The `word_tokenize` breaks about words that have punctuation built into them.\n",
    "\n",
    "It would be similar to making our own punctuation list and cleaning each word, but it's quicker and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in set(char_nonstop['IAGO.']):\n",
    "    print(nltk.word_tokenize(word))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The value of this really comes into play when you haven't already done the processing work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize('the fox ran over the meadow, finding its prey.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And there are a large variety of tokenizers besides the word tokenizer. One of the most useful is the Regex tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_regex = nltk.RegexpTokenizer('\\w+')\n",
    "nltk_regex.tokenize('the fox ran over the meadow, finding its prey.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in char_nonstop['IAGO.']:\n",
    "    print( nltk_regex.tokenize(word) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So all of the processing work that we did ourselves from the character dialogue to words could have been handled by NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def nltk_cleaner(wordlist, charname):\n",
    "    charstop = stopWords.union(set([charname, charname.strip('.')]))\n",
    "    return [w for w in nltk_regex.tokenize(' '.join(wordlist)) if w not in charstop]\n",
    "\n",
    "nltk_cleaner(char_dialogue['IAGO.'], 'IAGO.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "char_nltk = {}\n",
    "for char in char_dialogue:\n",
    "    nltk_cleaner(char_dialogue[char], char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tokenization only breaks the string into word 'tokens' individual sets of characters, separated by a space or punctuation. This doesn't account for variation in words, due to conjugation or plural, that make them appear different when really they have the same underlying meaning. This process is what is known as **stemming**. \n",
    "\n",
    "When we stem a word, we remove the suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print( ps.stem('running') )\n",
    "print( ps.stem('runs') )\n",
    "\n",
    "print( ps.stem('party') )\n",
    "print( ps.stem('parties') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And we could add this to our `nltk_cleaner` function now to automatically stem the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_cleaner(wordlist, charname):\n",
    "    charstop = stopWords.union(set([charname, charname.strip('.')]))\n",
    "    return [ps.stem(w) for w in nltk_regex.tokenize(' '.join(wordlist)) if w not in charstop]\n",
    "\n",
    "char_nltk = {}\n",
    "for char in char_dialogue:\n",
    "    char_nltk[char] = nltk_cleaner(char_dialogue[char], char)\n",
    "char_nltk['IAGO.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IAGO', len( set(char_nltk['IAGO.']) ))\n",
    "print('OTHELLO', len( set(char_nltk['OTHELLO.']) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post tokenization and stemming we have achieved a decrease in the number of unique words used per character (these counts are roughly 75% of what they were before). At this point we should again check the rank frequency plot to understand how this has changed the behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (6, 3))\n",
    "#IAGO plot\n",
    "ax1 = fig.add_subplot(121, facecolor='white')\n",
    "rank_plot(ax1, 'IAGO', freq_calc(char_nltk['IAGO.']) )\n",
    "#OTHELLO plot\n",
    "ax2 = fig.add_subplot(122, facecolor='white')\n",
    "rank_plot(ax2, 'OTHELLO', freq_calc(char_nltk['OTHELLO.']) )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So far we have been just doing a raw count - that doesn't really deal with the diversity of language.\n",
    "\n",
    "The most common and straightforward implementation of that is to look at the uniqueness of words used in comparison to the total number of words used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Even with this simple calculation, we can see that while Iago may speak more - more of it is repeated utterances than Othello.\n",
    "\n",
    "**Question** Is this difference significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Apparent numbers (can) lie.**\n",
    "\n",
    "This is the absolute importance of null models when dealing with data, such as text data, that breaks a classical statistical testing framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extending beyond a bag of words\n",
    "\n",
    "So far we have only dealt with what are called `uni-grams` (i.e. single word, bag of words). Bigrams and trigrams are also a part of the picture. can you guess what they are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nltk.bigrams(char_nltk['IAGO.']))[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why do bigrams matter? Typically it is in the context of some other analysis or relationship (i.e. some statistical learning module as a feature vector). They handicap your ability to do a direct analysis (two items instead of one); however, they expand your ability to model structure.\n",
    "\n",
    "There are a number of reasons and instances where you will want to rely on Xgrams instead of or in addition to the unigram bag of words approach in order to have a description of the text that takes structure into account. A common reason is when you have a multi-word concept that encodes meaning, which often happens in specialized fields/writing. \n",
    "\n",
    "Just because you perform the extraction as multiple words, doesn't mean that you cannot reduce the Xgram  to a single word token. For convenience in further processing these tokens will be joined as a single string like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['-'.join(x) for x in list(nltk.bigrams(char_nltk['IAGO.']))[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You wouldn't join unigrams and bigrams in a bag-of-words analysis, but you would do so in applications where you are generating features from the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# POS\n",
    "\n",
    "The final basic? analysis is determining the parts of speech. This is the lead in to may other machine learning techniques that leverage parts of speech to determine structure and novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(nltk_regex.tokenize('The fox ran quickly to its prey'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this work? The simple answer is that this is a pre-trained model built from an annotated corpus. Text from that corpus has been trained like so:\n",
    "\n",
    "`[[('today','NN'),('is','VBZ'),('good','JJ'),('day','NN')], [('yes','NNS'),('it','PRP'),('beautiful','JJ')]]`\n",
    "\n",
    "and the model takes that learning and makes predictions on newly submitted text. The NLTK pos_tagger is trained on The Wall Street Journal corpus (clear links to source here: https://stackoverflow.com/questions/32016545/how-does-nltk-pos-tag-work/41384824)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy\n",
    "\n",
    "Of course NLTK isn't for cool kids anymore. Now it's all about spacy. Spacy has pretrained statistical language models and an opinionated implemention of a NLP pipeline that is easy to use. \n",
    "\n",
    "Normally we would have to download one of these language models like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we already did this at the start of class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp('The fox ran quickly to its prey')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course what makes everyone really care about spacy is the fact that it has that statistical model of language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Apple is looking at buying U.K. startup for $1 billion')\n",
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the fact that it can handle modern web text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Pensive emoji is where it has always been. Pensive is the superior \"\n",
    "          \"emoji. It's outranking happy ðŸ˜” \")\n",
    "print(doc[0].text)          \n",
    "print(doc[1].text)          \n",
    "print(doc[-1].text)         \n",
    "print(doc[17:19].text)      \n",
    "\n",
    "noun_chunks = list(doc.noun_chunks)\n",
    "print(noun_chunks[0].text)  \n",
    "\n",
    "sentences = list(doc.sents)\n",
    "print(sentences[1].text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
