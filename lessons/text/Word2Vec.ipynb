{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The purpose\n",
    "\n",
    "In any language there is structure in a sentence; verbs communicate action, nouns represent entities, and adjectives describe.\n",
    "\n",
    "For a single word, synonyms represent a word that has the same or nearly the same meaning that can serve as a substitute. These relationships are curated/stored manually in thesauruses.\n",
    "\n",
    "However, there are relationships between words that are not synonyms - i.e. both a car and a horse can be means of conveyance (although a horse also exists on its own, unlike a car). What we desire is to build a framework that allows us to identify similarities in words and their meanings that extends beyond substitutable words.\n",
    "\n",
    "This effort/project would be far too large to be undertaken by humans, the question is how we could quantitatively tackle it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exploiting structure\n",
    "\n",
    "Fortunately, languages are constructed as such that there is some similarity in sentence structure and word usage when we attempt to express similar concepts.\n",
    "\n",
    "*I rode in the car into town.* \n",
    "\n",
    "*I rode the horse into town.*\n",
    "\n",
    "Not **exactly** the same, which makes this task difficult, but sufficiently similar to undertake the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vectors and matrices - verbal similarity to numeric distance\n",
    "\n",
    "The basic concept - take words and according to how they are used in bodies of text, encode them in a n-dimensional space. Since there is no starting cartesian coordinate for a word, we have to essentially situate all of the words based on their co-occurences in sentences together. Words that are never together shouldn't be right next to each other, but if they are used in sentences similarly then the direction/magnitude of their relationships should be similar (although distant in vector space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8 + 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='../../images/word_matrix.jpg' width='400px'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting context into numbers\n",
    "\n",
    "There multiple approaches for encoding verbal context into numeric distances, we are going to explore *k-skip n-grams*, which is one of the approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-skip n-grams\n",
    "\n",
    "Once we detail it, then name gives it away. A k-skip n-gram specifies that you will create all n-grams in a sentence up to k-skips away. Similar to a rolling average, k is our window length. So if we take the sentence:\n",
    "\n",
    "*I ran to the store to pick up milk.*\n",
    "\n",
    "and said that we should do **bi-grams**, then we would have\n",
    "\n",
    "`[('I', 'ran'), ('ran', 'to'), ('the', 'store'), ('store', 'to'), ('to', 'pick'), ('pick', 'up'), ('up', 'milk')]`\n",
    "\n",
    "which is 7 bi-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I then asked for **1-skip bi-grams**, then we would have:\n",
    "\n",
    "`[(I, ran), (I, to), (ran, to), (ran, the), (to, the), (to, store), (the, store), (the, to), (store, to), (store, pick), (to, pick), (to, up), (pick, up), (pick, milk), (up, milk)]`\n",
    "\n",
    "which is 15 bi-grams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use any combination of `n` and `k` that you desire, but it is already to easy to see how the number of word combinations explodes as `n` and `k` increase for even a single sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import skipgrams\n",
    "\n",
    "sentence = ['I', 'ran', 'to', 'the', 'store', 'to', 'pick', 'up', 'milk']\n",
    "list( skipgrams(sentence, 2, 0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list( skipgrams(sentence, 2, 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list( skipgrams(sentence, 2, 2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list( skipgrams(sentence, 3, 2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relatively simple to calculate quickly with nltk. Writing our own function wouldn't be awful either, we would just want to make use of the `collections` library and use the combination/permutation functions. \n",
    "\n",
    "As a quick test, I am want you to load one the WoS data and calculate how many skip-grams there are for one journal (take your pick of `n` and `k`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise\n",
    "import json\n",
    "wosdata = json.load(open('../data/wos_topic_doc.json'))\n",
    "\n",
    "wosdata.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for skipn in [0, 1, 2, 3, 4]:\n",
    "    skip_count = 0\n",
    "    for entry in wosdata['AMERICAN_ECONOMIC_REVIEW']:\n",
    "        skip_count += len( list(skipgrams(entry, 4, skipn)) )\n",
    "    print(skipn, skip_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wosdata['AMERICAN_ECONOMIC_REVIEW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does this fit in?\n",
    "\n",
    "The trick is that we turn this into a prediction problem. When we look at the sentence\n",
    "\n",
    "`I ran to the store to pick up milk.`\n",
    "\n",
    "and we construct a skip-gram\n",
    "\n",
    "`['ran', 'to', 'store']`\n",
    "\n",
    "there is the skipped word\n",
    "\n",
    "`'the'`\n",
    "\n",
    "which we could set up as a `(context, target)` pair\n",
    "\n",
    "`(['ran', 'to', 'store'], 'the')`\n",
    "\n",
    "We could set this up as a problem when we input `['ran', 'to', 'store']` to predict `the` or use `the` to predict its context words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the prediction problem\n",
    "\n",
    "There are two ways that we can go about this - one is to use the skip grams model we already set up\n",
    "\n",
    "<img src='../../images/skipgram.png'></img>\n",
    "\n",
    "the embedding then is the learned weights in the projection layer.  This works best when we have a very large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternative is the continuous bag of words\n",
    "\n",
    "<img src='../../images/cbow.png'></img>\n",
    "\n",
    "with this approach, we use the context to predict the target. This averages out the noise of individual words contribution to context, which makes it workable with smaller datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A primer on neural nets\n",
    "\n",
    "<img src='../../images/l9_neuralnet_-1.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='../../images/l9_neuralnet_0.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='../../images/l9_neuralnet_1.png' width='600px'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../images/l9_neuralnet_2.png' width='600px'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../images/l9_neuralnet_3.png' width='600px'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../images/l9_neuralnet_4.png' width='600px'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating error in a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Youtube\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Ilg3gGewQ5U?rel=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec in Gensim\n",
    "\n",
    "We can create our own word2vec model in tensorflow and train it, but gensim also has a built-in implementation that requires less boilerplate code and still retains most options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [x.lower().split() for x in [\"Human machine interface for lab abc computer applications\",\n",
    "                                         \"A survey of user opinion of computer system response time\",\n",
    "                                         \"The EPS user interface management system\",\n",
    "                                         \"System and human system engineering testing of EPS\",              \n",
    "                                         \"Relation of user perceived response time to error measurement\",\n",
    "                                         \"The generation of random binary unordered trees\",\n",
    "                                         \"The intersection graph of paths in trees\",\n",
    "                                         \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "                                         \"Graph minors A survey\"]]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim as gs\n",
    "#and we can create a model in one shot if we wanted to.\n",
    "model = gs.models.Word2Vec(sentences, min_count=1, sg=0, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it to get up and running. `sg=1` sets Word2Vec to use skip-grams, while `sg=0` sets Word2Vec to use continuous bag of words. \n",
    "\n",
    "We could double check what the vector size is (although this is set when we initialize the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check on the vocabulary constructed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And using one of the words, pull its vector out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv['trees']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And ask it to predict a word given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_output_word(['graph', 'trees'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could pull out the vector weigths for all of the vocabulary words and attempt to look at them also.\n",
    "\n",
    "Pull out all of the words and plot the first two dimensions with the word labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the maximum variance\n",
    "\n",
    "Picking two dimensions at random isn't a good way to visualize the embedding, since the algorithm is relying on all 100 dimensions to describe the words (i.e. all dimensions are working to create the optimal embedding).\n",
    "\n",
    "Since we lack the ability to plot/view 100 dimensions simultaneously our best bet is to employ **dimensionality reduction** and create optimally weighted axes across dimensions to capture the variance within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a commonly used dimensionality reduction algorithm. It works by transforming a set of data observations into a set of linearly uncorrelated variables (the components). The difference between PCA and Factor Analysis is that the components are orthogonal (i.e. components are at 90 degree angles from one another). The calculation of these components is based on the eigenvectors of the data matrix (which is also what guarantees us that the components are orthogonal).\n",
    "\n",
    "We can use the scikit learn implementation of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving forward\n",
    "\n",
    "And this is as far as we can possibly take this example text. To **actually** train a word embedding, you need a massive dataset. \n",
    "\n",
    "In lieu of that, we can load up trained representations that someone else has already done. We will use the Stanford Global Vectors for Word Representation (GloVe), since it is a smaller file size than Google's trained word2vec.\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "You can move the downloaded folder into our `data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ../data/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four files that GloVe supplies, they were all trained on the same corpus but with different sized vectors (vector size is marked in the filename). \n",
    "\n",
    "We will keep working with the 100 dimension vector. Unlike the google datasets (which are in a standard word2vec format), we need to convert the GloVe dataset to a word2vec format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = '../data/glove/glove.6B.100d.txt'\n",
    "word2vec_output_file = '../data/glove/glove.6B.100d.word2vec.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can read this word2vec file the same as we could one that we downloaded directly from Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a large, well-trained dataset we can start pulling out the famous examples also.\n",
    "\n",
    "`king - man + woman = ?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = glove_model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## How does this work?\n",
    "\n",
    "Everything is based off of vectors. Given two data points, they each have a vector from the origin.\n",
    "\n",
    "<img src='../../images/l19_vector_origin.jpg' width='300px'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector addition works by stacking the vectors\n",
    "\n",
    "If we add `King+Man`, then we have a resultant vector that is the length of both.\n",
    "\n",
    "<img src='../../images/l19_vector_addition.jpg' width='300px'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector subtraction requires 'flipping' the subtracted vector\n",
    "\n",
    "<img src='../../images/l19_vector_subtraction.jpg' width='300px'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity between two vectors is based off of the angle between them\n",
    "\n",
    "<img src='../../images/l19_cosine_1.jpg' width='300px'></img>\n",
    "<img src='../../images/l19_cosine_2.jpg' width='300px'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar(positive=['cat', 'puppy'], negative=['kitten'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar(positive=['dog', 'kitten'], negative=['puppy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When testing these relationships, they work by essentially asking for a relationship with the missing word in the 'negative' list and it doesn't work any which way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar(positive=['dog', 'puppy'], negative=['cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further extend this 'play' with other examples. If we enter in one word then we are effectively asking for the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar(positive=['frog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar(positive=['puppy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there can be interesting matches given a multi-word concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar(positive = ['slow', 'slower', 'slowest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do you test the 'goodness'\n",
    "\n",
    "Despite the fact that we have set this problem up as a *prediction* one to fit the optimal weights in the neural network, this is still an unsupervised problem (i.e. we are choosing that the nearby context words are either the input or the output).\n",
    "\n",
    "The **best** test that we possibly have is to see if our trained weights can replicate already agreed upon relationships.\n",
    "\n",
    "As a part of the original word2vec project, they supplied `questions-words.txt` which has a large number of these analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [line.strip() for line in open('../data/questions-words.txt').readlines() if ':' in line]\n",
    "headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of relationships are encoded in the lines that start with a ':'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [l.lower().strip().split() for l in open('../data/questions-words.txt').readlines() if ':' not in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can test to see if the trained embeddings can reproduce the expected relationships.\n",
    "\n",
    "All of the lines are set up as \n",
    "\n",
    "`x2 - x1 + x3 = x4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in phrases[:10]:\n",
    "    print(glove_model.most_similar(positive=[p[1], p[2]], negative=[p[0]], topn=1), p[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this using the `accuracy` function off of a word2vec model to test the entire accuracy for the whole dataset. It will automatically separate the test set by section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = glove_model.accuracy('../data/questions-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h, section in zip(headers, results):\n",
    "    print(len(section['correct'])/(len(section['correct']) + len(section['incorrect'])), h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test a non-standard embedding, you would need to establish these analogies yourself. Given the number of basic parameters you can tweak (including the window size and training speed), there is a distinct need to test the goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Expanding beyond words\n",
    "\n",
    "Moving beyond a word to sentence is a more complex task than it may appear, since it must also encode the word structure within the sentence. \n",
    "\n",
    "Using word2vec, we could create the average vector over all words in a sentence and then search for similar average vectors from other words. However, there is an extension of `word2vec`, `doc2vec`, that is included in gensim that works much better at the phrase, paragraph, and document level."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
