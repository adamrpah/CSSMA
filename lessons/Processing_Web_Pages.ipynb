{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def css_styling():\n",
    "    styles = open(\"../data/www/styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Beautiful Soup, so rich and green, waiting in a hot tureen!\n",
    "\n",
    "(*The Lobster Quadrille*, Alice in Wonderland)\n",
    "\n",
    "We are now ready to start scraping web pages. In order to do so we are going to use [`BeautifulSoup`](http://www.crummy.com/software/BeautifulSoup/bs4/doc/), a powerful python package to parse web pages you already scraped. Normally you would use `requests` (to GET the page) and then `BeautifulSoup` to analyse the web page.\n",
    "\n",
    "We will use the wikipedia page for a player from Germany's national football team as an example: https://en.wikipedia.org/wiki/Erik_Durm that has already been downloaded into the `Data/` folder. We are starting with a pre-downloaded HTML page so that there aren't a hundred requests from the same place for the same page at the same server at the same time from (which will frequently result in you getting blocked from accessing that website!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by opening up the page and convert it to a `soup` object. Then, we're going to use the `find` method to find the page's `<title>` tag and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/erik_durm_wiki.html\", \"r\", encoding=\"utf-8\") as wiki_file:\n",
    "        soup = bs4.BeautifulSoup(wiki_file.read(), 'lxml')\n",
    "\n",
    "#The soup is the entire page\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are a number of different functions of a soup\n",
    "dir(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We're going to start with the `find` function. It will find the first tag of the given type.\n",
    "title = soup.find('title')\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the title is the entire html tag. If we want only the text within it, then we need to ask for the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for this is that Beautiful Soup converts HTML tags into its own `Tag` objects.`Tag` objects have many useful attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(title))\n",
    "print(title.text) # The text gives you the visible part of the tag\n",
    "print(title.name) # The type of tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a tag has any html attributes, they can be accessed in a very \"pythonic\" way. That is, they are organized as a dictionary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = soup.find(\"h1\")\n",
    "\n",
    "print(h1.attrs)\n",
    "print(h1[\"class\"])\n",
    "print(h1[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of searching for `Tags` one by one, we can also retrieve them all at once.  As an example, let's find all level 2 headers. To this end, we use the `find_all` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = soup.find_all('h2')\n",
    "\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too much information!  In order to get the only the information that we need, we must restrict to the desired attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for header in headers:\n",
    "    print(header.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another `Tag` that that is useful and that demonstrate some of the other useful attributes is the one for webpages that our page points to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('a')\n",
    "\n",
    "for link in links[:10]:  # Showing just the first 10 links for brevity\n",
    "    # href represents the target of the link\n",
    "    # Where the link actually goes to!\n",
    "    print('-----', link.text)\n",
    "    print(link.get('href'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching using attribute information\n",
    "\n",
    "Some `Tag` elements have attributes associated with them. These includes `id`, `class_`, `href`.  Our search can restrict results to attributes with a specific value or to results where the attribute type is included.\n",
    "\n",
    "Note that we must use `class_` instead of `class` to avoid conflicts with Python's built-in keyword. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the element with the attribute \"id\" equal to \"Early_career\"\n",
    "tag = soup.find(id=\"Early_career\")\n",
    "print(tag)\n",
    "print(tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all elements with an href attribute\n",
    "all_links = soup.find_all(href=True)\n",
    "print(len(all_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve inline citations -- they are <sup> elements with the class \"reference\"\n",
    "soup.find_all(\"sup\", class_=\"reference\")[5:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all tags with class=mw-headline and an id attribute (regardless of value)\n",
    "soup.find_all(attrs={\"class\": \"mw-headline\", \"id\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigating the HTML tree \n",
    "\n",
    "\n",
    "Besides being able to search elements anywhere on the whole html tree, beautiful soup also allows you to navigate the tree in any direction.\n",
    "\n",
    "Let's try to get at the first paragraph (`<p>`) in the `Club career` section starting from the section's title tag.\n",
    "\n",
    "Here's the relevant HTML snippet:\n",
    "\n",
    "```html\n",
    "    <h2>\n",
    "      <span class=\"mw-headline\" id=\"Club_career\">Club career</span>\n",
    "      <span class=\"mw-editsection\">\n",
    "        <span class=\"mw-editsection-bracket\">[</span>\n",
    "        <a href=\"/w/index.php?title=Erik_Durm&amp;action=edit&amp;section=1\" title=\"Edit section: Club career\">edit</a>\n",
    "        <span class=\"mw-editsection-bracket\">]</span>\n",
    "      </span>\n",
    "    </h2>\n",
    "    <h3>\n",
    "      <span class=\"mw-headline\" id=\"Early_career\">Early career</span>\n",
    "      <span class=\"mw-editsection\">\n",
    "        <span class=\"mw-editsection-bracket\">[</span>\n",
    "        <a href=\"/w/index.php?title=Erik_Durm&amp;action=edit&amp;section=2\" title=\"Edit section: Early career\">edit</a>\n",
    "        <span class=\"mw-editsection-bracket\">]</span>\n",
    "      </span>\n",
    "    </h3>\n",
    "    <p>Durm began his club career in 1998 at the academy of SG Rieschweiler....</p>\n",
    "```\n",
    "\n",
    "We can see that that section of text is *under* the \"Club career\" title: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_headline = soup.find(id=\"Club_career\")\n",
    "print(section_headline)\n",
    "print(section_headline.text)\n",
    "section_headline.contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `contents` attribute lets us access everything that is inside a given tag. In this case we find only the visible text of the tag.\n",
    "\n",
    "Looking at the webpage snippet, we see that the tag `<p>` is at the same level as the tags `<h2>` and `<h3>`.  Hence, we need to navigate up one level (to the `<h2>` tag), then navigate to its second sibling (first `<h3>` then `<p>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_h2 = section_headline.parent  # Up one level\n",
    "print( parent_h2.name == \"h2\" )      # Is it the <h2> tag?\n",
    "print()\n",
    "print(parent_h2.contents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step = parent_h2.next_sibling\n",
    "print(one_step.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_steps = one_step.next_sibling\n",
    "print(two_steps.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only at the `<h3>` tag even though we moved past two siblings.  The reason is that some of the siblings in the soup are not actual HTML elements. Some could simply be empty lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_steps = two_steps.next_sibling\n",
    "print(three_steps.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_steps = three_steps.next_sibling\n",
    "print(four_steps.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(four_steps.contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Now we are where we wanted to be. We have the text corresponding to the `<p>` tag.  This is something we must always be mindful about. Web scraping can, and very frequently will be, messy and will involve trial-and-error...\n",
    "\n",
    "We can the contents of our desired element is a list.  Let's obtain the number of elements and check what they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(four_steps.contents))\n",
    "print(four_steps.contents[1])\n",
    "print(four_steps.contents[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much nicer!\n",
    "\n",
    "Besides the `find_next_sibling` method, there are also `find_previous_sibling`, `find_next_children`, `find_previous_children`, and many others.\n",
    "\n",
    "The [Beautiful Soup documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) has a comprehensive list of all these methods. There is no need to memorize all of them. It's more important to realize that, as with any programming language, there is more than one way to get any element of the html tree. The trick is to *pick a good starting point* from where to start the scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping images\n",
    "\n",
    "You can also use Beautiful Soup to get the source of an image from a webpage. It works just the same as for text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some modules that will allows us to display images and other media in the notebook itself\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in soup.find_all('img'):\n",
    "    print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pinpoint a specific image and get its attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = soup.find_all('img')\n",
    "img0 = images[0]\n",
    "print(img0.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can display the image using its `src` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(url='../data/' + img0['src']))\n",
    "\n",
    "display(Image(url='../data/' + images[1]['src']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Scraping results from your Personality profile\n",
    "\n",
    "For this exercise you will use your results from the personality quiz at [HEXACO](http://hexaco.org/hexaco-online). You did take the quiz right? :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/my_hexaco.html\", \"r\", encoding=\"utf-8\") as hexaco_file:\n",
    "        soup = bs4.BeautifulSoup(hexaco_file.read(), 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Find the `<table>` element, that contains your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 -  Find all the scale names using the `table` variable from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Now get both the scale names and your own scores associated with each scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Now replot your scores as a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
