{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Beautiful Soup, so rich and green, waiting in a hot tureen!\n",
    "\n",
    "(*The Lobster Quadrille*, Alice in Wonderland)\n",
    "\n",
    "We are now ready to start scraping web pages. In order to do so we are going to use [`BeautifulSoup`](http://www.crummy.com/software/BeautifulSoup/bs4/doc/), a powerful python package to parse web pages you already scraped. Normally you would use `requests` (to GET the page) and then `BeautifulSoup` to analyse the web page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by opening up the page and convert it to a `soup` object. Then, we're going to use the `find` method to find the page's `<title>` tag and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/classdata/company_search/Amazon_query.html\", \"r\", encoding=\"utf-8\") as infile:\n",
    "        soup = bs4.BeautifulSoup(infile.read())\n",
    "\n",
    "#The soup is the entire page\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are a number of different functions of a soup\n",
    "dir(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We're going to start with the `find` function. It will find the first tag of the given type.\n",
    "title = soup.find('title')\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the title is the entire html tag. If we want only the text within it, then we need to ask for the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for this is that Beautiful Soup converts HTML tags into its own `Tag` objects.`Tag` objects have many useful attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(title))\n",
    "print(title.text) # The text gives you the visible part of the tag\n",
    "print(title.name) # The type of tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a tag has any html attributes, they can be accessed in a very \"pythonic\" way. That is, they are organized as a dictionary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find(\"table\")\n",
    "\n",
    "print(table.attrs)\n",
    "print(table[\"class\"])\n",
    "print(table[\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of searching for `Tags` one by one, we can also retrieve them all at once.  As an example, let's find all level 2 headers. To this end, we use the `find_all` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = soup.find_all('div')\n",
    "\n",
    "print(divs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too much information!  In order to get the only the information that we need, we must restrict to the desired attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for div in divs:\n",
    "    print(div.text[:100])\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another `Tag` that that is useful and that demonstrate some of the other useful attributes is the one for webpages that our page points to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('a')\n",
    "\n",
    "for link in links[:10]:  # Showing just the first 10 links for brevity\n",
    "    # href represents the target of the link\n",
    "    # Where the link actually goes to!\n",
    "    print('-----', link.text)\n",
    "    print(link.get('href'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching using attribute information\n",
    "\n",
    "Some `Tag` elements have attributes associated with them. These includes `id`, `class_`, `href`.  Our search can restrict results to attributes with a specific value or to results where the attribute type is included.\n",
    "\n",
    "Note that we must use `class_` instead of `class` to avoid conflicts with Python's built-in keyword. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the element with the attribute \"id\" equal to \"breadCrumbs\"\n",
    "tag = soup.find(id=\"breadCrumbs\")\n",
    "print(tag)\n",
    "print(tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all elements with an href attribute\n",
    "all_links = soup.find_all(href=True)\n",
    "print(len(all_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve company match class \n",
    "soup.find_all(class_=\"companyMatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all tags with class=blueRow and and no id attribute\n",
    "soup.find_all(attrs={\"class\": \"blueRow\", \"id\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigating the HTML tree \n",
    "\n",
    "\n",
    "Besides being able to search elements anywhere on the whole html tree, beautiful soup also allows you to navigate the tree in any direction.\n",
    "\n",
    "Let's navigate to the first row result in the table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find(class_=\"tableFile2\")\n",
    "print(table)\n",
    "print(table.text)\n",
    "table.contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `contents` attribute lets us access everything that is inside a given tag. In this case we find only the visible text of the tag.\n",
    "\n",
    "Looking at the webpage snippet, we see that the tag `<tr>` is at the same level as all other `<tr>` tags, and they are all children of the table object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_row = table.find('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead_row = tr_row.next_sibling\n",
    "dead_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row = dead_row.next_sibling\n",
    "print(data_row.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've had to traverse two siblings to get to the data row.  The reason is that some of the siblings in the soup are not actual HTML elements. Some could simply be empty lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_steps = data_row.next_sibling\n",
    "print(three_steps.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_steps = three_steps.next_sibling\n",
    "print(four_steps.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(four_steps.contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the `find_next_sibling` method, there are also `find_previous_sibling`, `find_next_children`, `find_previous_children`, and many others.\n",
    "\n",
    "The [Beautiful Soup documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) has a comprehensive list of all these methods. There is no need to memorize all of them. It's more important to realize that, as with any programming language, there is more than one way to get any element of the html tree. The trick is to *pick a good starting point* from where to start the scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping images\n",
    "\n",
    "You can also use Beautiful Soup to get the source of an image from a webpage. It works just the same as for text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some modules that will allows us to display images and other media in the notebook itself\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/erik_durm_wiki.html\", \"r\", encoding=\"utf-8\") as infile:\n",
    "        soup = bs4.BeautifulSoup(infile.read())\n",
    "\n",
    "for image in soup.find_all('img'):\n",
    "    print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pinpoint a specific image and get its attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = soup.find_all('img')\n",
    "img0 = images[0]\n",
    "print(img0.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can display the image using its `src` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(url='../../data/' + img0['src']))\n",
    "\n",
    "display(Image(url='../../data/' + images[1]['src']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
