{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative process for LDA\n",
    "\n",
    "####  1. Generate the topic-document distribution  $\\Theta _{d*}$ \n",
    "\n",
    "The probabilistic generative process for LDA to generate a collection of documents makes the following assumptions.\n",
    "\n",
    "For document $d $, the topic-document distribution  $\\Theta _{d*}$  is chosen according to the  Dirichlet distribution\n",
    "\\begin{equation} \\label{eq_lda02}\n",
    "\t\t\t\\quad p\\left( {{\\Theta _{d*}}|\\vec \\alpha } \\right) \n",
    "\t\t\t= \\mathscr{D} (\\vec \\alpha) \n",
    "\t\t\t= \\frac{{  \\Gamma \\left( {\\sum\\nolimits_{z = 1}^K {{\\alpha _z}} } \\right)   }}{{   \\prod\\nolimits_{z = 1}^K {\\Gamma \\left( {{\\alpha _z}} \\right)}   }}\\prod\\limits_{z = 1}^K {\\Theta _{dz}^{{\\alpha _z} - 1}} ,\\; \\forall d  \\in \\{1,...,M\\}  \\; ,\n",
    "\\end{equation}\n",
    "where $\\vec \\alpha = \\{\\alpha_z\\}_{k=1}^K$ is a vector and denotes the concentration hyperparameter  for  Dirichlet distribution;  \n",
    "and $\\Theta _{d*}$  is a multinomial distribution depending on the output of Dirichlet distribution $\\mathscr{D}(\\vec  \\alpha)$. \n",
    "A special case typically considered in LDA is a symmetric Dirichlet distribution, where   $\\alpha_z = \\alpha, \\forall z \\in \\{1,...,K\\}$.\n",
    "\n",
    "#### 2. Generate the word-topic distribution $\\Phi _{z*}$\n",
    "\n",
    "For  topic $z$,  the word-topic distribution $\\Phi _{z*}$  is also generated according to  Dirichlet distribution\n",
    "\\begin{equation} \\label{eq_lda01}\n",
    "\\quad p\\left( {{\\Phi _{z*}}|\\vec \\beta } \\right) \n",
    "=\\mathscr{D} ( \\vec \\beta ) \n",
    "=  \\frac{{    \\Gamma \\left( {\\sum\\nolimits_{v = 1}^W {{\\beta_v}} } \\right)    }}{{   \\prod\\nolimits_{v = 1}^W {\\Gamma \\left( {{\\beta_v}} \\right)}     }}\\prod\\limits_{v = 1}^W {\\Phi _{zv}^{\\beta_v  - 1}}\n",
    ",\\; \\forall z  \\in \\{1,...,K\\}  \\; ,\n",
    "\\end{equation}\n",
    "where $\\vec \\beta = \\{\\beta_v\\}_{v=1}^W$ is a vector and denotes the concentration hyperparameter  for the  Dirichlet distribution; \n",
    "and $\\Phi _{z*}$  is a multinomial distribution depending on the output of  Dirichlet distribution $\\mathscr{D}( \\vec \\beta)$.\n",
    "Note that a symmetric Dirichlet distribution is usually considered for convenience, in which $\\beta_v = \\beta$ for all $v$.\n",
    "\n",
    "#### 3. Choose a topic for each token\n",
    "\n",
    "Having defined these distributions, the  topic $z_{dt}$ of the token $w_{dt}$  is chosen according to the topic distribution $\\Theta_{d*}$,\n",
    "\\begin{equation} \\label{eq_lda03}\n",
    " \t{z_{dt}} \\sim \\mathscr{M}\\left( \\Theta_{d*} \\right) = p\\left( {{z_{dt}} = z|\\Theta _{d*}} \\right) \\; ,\n",
    "\\end{equation}\n",
    "where $\\mathscr{M}$ means multinomial distribution.\n",
    "\n",
    "#### 4. Choose a word type for each token\n",
    "\n",
    "A word $v$ is chosen for each token $w_{dt}$  according to the topic of the token $z_{dt}$ and the term distribution of each topic $\\Phi_{z*}$, \n",
    "\\begin{equation} \\label{eq_lda04}\n",
    "{w_{dt}} \\sim \\mathscr{M}\\left( {\\Phi_{{z_{dt},*}}} \\right) = p\\left( {{w_{dt}} = v|{\\Phi_{z_{dt},*}}} \\right)\\; .\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Posterior distribution of the corpus\n",
    "\n",
    "Given a document with the BoW matrix $\\mathbf{X}$, the posterior distribution of the hidden variable can be calculated as \n",
    "\\begin{equation} \\label{eq_lda05}\n",
    "\\begin{array}{l}\n",
    "p\\left( { \\mathbf{\\Phi} , \\mathbf{\\Theta} | \\mathbf{X}, \\vec \\alpha , \\vec \\beta } \\right) \n",
    "= \\frac{   p\\left(  \\mathbf{X} |  \\mathbf{\\Phi} , \\mathbf{\\Theta}  \\right)   p\\left(    \\mathbf{\\Phi} , \\mathbf{\\Theta}  | \\vec \\alpha , \\vec \\beta   \\right)  }  { p\\left( {   \\mathbf{X}  | \\vec \\alpha , \\vec \\beta } \\right)  } \\; .\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "where  $ p\\left(  \\mathbf{X} |  \\mathbf{\\Phi} , \\mathbf{\\Theta}  \\right)  $ is the joint likelihood \n",
    "$\\prod\\limits_{d = 1}^M \\prod\\limits_{v = 1}^{W}  \\left(\\frac{n_d} {n_M} \\sum\\limits_{z = 1}^K \\Theta _{dz}\\Phi _{zv} \\right)  ^{X_{vd}} \\; $; \n",
    "$ p\\left(    \\mathbf{\\Phi} , \\mathbf{\\Theta}  | \\vec \\alpha , \\vec \\beta   \\right) = \\prod\\nolimits_z p\\left( {{\\Phi _{z*}}|\\vec \\beta } \\right) \\prod\\nolimits_d p\\left( {{\\Theta _{d*}}|\\vec \\alpha } \\right)   $, is the prior distributions ; \n",
    "$  p\\left( {   \\mathbf{X}  | \\vec \\alpha , \\vec \\beta } \\right) = \\int {d\\mathbf{\\Phi}} \\int {d\\mathbf{\\Theta}} p\\left(  \\mathbf{X} |  \\mathbf{\\Phi} , \\mathbf{\\Theta}  \\right)   p\\left(    \\mathbf{\\Phi} , \\mathbf{\\Theta}  | \\vec \\alpha , \\vec \\beta   \\right) $,  is the evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference of the latent topic structure\n",
    "\n",
    "The finall aim is to get the value of the topic-document distribution  $\\Theta$ and the  word-topic distribution $\\Phi$.\n",
    "\n",
    "Theoretically, we can determine the values of the latent variables via maximization of the posterior distribution. \n",
    "However, the posterior distribution cannot be computed directly for a exact solution due to its complexity and the large state space. \n",
    "As a result, there are a set of estimation algorithms proposed for approximating latent variables in LDA model.\n",
    "The two most widely used numerical methods are the Gibbs sampling algorithm and the variational inference method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It sounds complicated. But gensim can help us to handle the inference process nicely. And gensim uses the variational inference method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
