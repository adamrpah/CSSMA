{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Web scraping and crawling\n",
    "\n",
    "Now we're moving forward in terms of difficulty - writing code to traverse and capture data from the web.\n",
    "\n",
    "You largely already have the skills necessary to do this, the major skill is being able to parse the structure and text of a HTML document. Now we are simply going to put together the mental map of how to instruct a program to walk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Orders of complexity\n",
    "\n",
    "There is an increasing level of difficulty in how one scrapes web pages and the intransigence of your target should be the determining factor in which approach you implement (i.e. don't buy a bazooka to go to a knife fight).\n",
    "\n",
    "* Exploiting regularly structured urls (`requests`)\n",
    "* Crawling a site with typically static content (`scrapy`)\n",
    "* Crawling a site with dynamic content and human restrictions (`selenium`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So let's continue - regularly structured urls\n",
    "\n",
    "To illustrate this approach, I want to use company financial filings since they contain a wealth of information. For any publicly traded company, you can access all of their filings through the [SEC Edgar website](https://www.sec.gov/edgar/searchedgar/companysearch.html).\n",
    "\n",
    "However, to access the filings you will need to have a company's CIK number (this is used to disambiguate companies). Fortunately, the SEC provides that search function for you.\n",
    "\n",
    "<img src='../images/edgar_search.png'>\n",
    "\n",
    "Now, the trick here is that once you press the search button and get the results you should check the url bar.\n",
    "\n",
    "<img src='../images/edgar_url.png'>\n",
    "\n",
    "Notice anything....pertinent? Repeatable?\n",
    "\n",
    "The trick is that you make sure that the url has your search query (`Google` in our case) in plain text - then modify the search term in place and try the new url. Does it work? If it does...you can 'scrape' any site easily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "I want you to scrape all the CIKs for the following list of companies. and save them to a folder you create in `../data/classdata/company_searches/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Exercise\n",
    "\n",
    "companies = ['Google', 'Zebra', 'Cisco', 'Oracle', 'Amazon']\n",
    "\n",
    "print(companies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And now with these CIKs I want you to pull all filing descriptions. Keep them associated with the CIK and save them to a file in a folder you create in `../data/classdata/sec_descriptions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pretty good! But one issue with our lazy scraping - what about pages that have more than 40 descriptions?\n",
    "\n",
    "And you could just as easily change this to follow the links and download the original documents that were filed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Crawling static content \n",
    "\n",
    "You could just as easily continue using requests and this type of logic to crawl an entire web page (find all `<a>` tags, follow them, track which urls have already been travelled to, etc.)\n",
    "\n",
    "Here we will work with a library called Scrapy. One of the benefits of Scrapy is that they have a cloud that you can deploy your scraper to. This helps avoid emotions like this:\n",
    "\n",
    "<img src='../images/whyyyyy.jpg' width='400px'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The downside of Scrapy is that it requires quite a bit of boilerplate to get going. A spider has to be initialized as a class that is an instance of `scrapy.Spider`. \n",
    "\n",
    "However, after that it is pretty simple. It will have one function, `parse` and emits them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class BlogSpider(scrapy.Spider):\n",
    "    name = 'blogspider'\n",
    "    start_urls = ['https://blog.scrapinghub.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        #Yields the title and url of a story\n",
    "        for title in response.css('h2.entry-title'):\n",
    "            yield {'title': title.css('a ::text').extract_first()}\n",
    "\n",
    "        #Yields a response follow object with the next page data\n",
    "        for next_page in response.css('div.prev-post > a'):\n",
    "            yield response.follow(next_page, self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now for the other pain - we actually can't run Scrapy code in the Jupyter notebook easily. You'll actually need to write it as a script (which I have alreaday done for you in this file [here](scrapy_example.py) )\n",
    "\n",
    "However, we can run the bash command to execute this file from the Jupyter notebook. We just need to put the `!` in front of it so the notebook shell knows that we are executing a bash command.\n",
    "\n",
    "We can also even store the output as a python variable to then interact with it in the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "blog_urls = !scrapy runspider scrapy_example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "blog_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Alternatively, you could save the urls to a file directly from the scraping code if you did not yield the titles and instead wrote it to a file. \n",
    "\n",
    "Writing a real spider is a bit more complicated and will require usage of python scripts and bash commands. I will attempt to do as much in the notebook as possible, but we will need to switch back and forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!scrapy startproject scraping_tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will simply follow the tutorial for now and scrape quotes from `quotes.toscrape.com` to be nice and not unleash 20 spiders on an unsuspecting website.\n",
    "\n",
    "So where to start? The first one is understanding the directory structure that was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!ls -l scraping_tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You have a config file (as noted by the `cfg` extension) and the directory that contains all of the code. Right now the config file will be pretty barren, but this is where you will add options for your project to manage the multiple scrapers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Right now the bigger deal is in the `scraping_tutorial` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ls -l scraping_tutorial/scraping_tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since this is a relatively simple scraper, we will really just care about adding spiders to the spiders folder (which is currently empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ls -l scraping_tutorial/scraping_tutorial/spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And now we have to create our Quotes spider. To do this you will need to start a python script `quotes_spider.py` in the `spiders/` folder. The url is `'http://quotes.toscrape.com/page/1/'` and all you need to accomplish is writing the body of the page into a html file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!cd scraping_tutorial && scrapy crawl quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!ls scraping_tutorial/\n",
    "\n",
    "!open scraping_tutorial/quotes-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And we can change up what we iterate through and instead of getting the page body and saving it, we could instead yield the quotes directly by selecting the appropriate css elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!cd scraping_tutorial && scrapy crawl quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can build far more complicated scrapers (scraping and processing scraped items), but this a sufficient introduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For the last part, we will tackle the most complicated approach - scraping dynamic content by impersonating a human with a real web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will need to download and install the `geckodriver` according to your system instructions (You will also need to move the `geckodriver` into `/usr/local/bin/` \n",
    "or `C:\\Windows\\System32\\`\n",
    "\n",
    "Now watch for something totally crazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!python selenium_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Yup, that's right. It started an entire web browser (Firefox in this case). This is why selenium is the most powerful (and costly) solution to scraping. \n",
    "\n",
    "So now let's inspect this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"http://www.python.org\")\n",
    "elem = driver.find_element_by_name(\"q\")\n",
    "elem.clear()\n",
    "elem.send_keys(\"pycon\")\n",
    "elem.send_keys(Keys.RETURN)\n",
    "assert \"No results found.\" not in driver.page_source\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You start from the webdriver with the browser of choice (you can choose). \n",
    "\n",
    "Using `driver.get()` you give a url address.\n",
    "\n",
    "Once there, you can give instructions to search for a specific element by it's name. In this case `q` is the input field for search the site.\n",
    "\n",
    "As a pre-emptive move, the code clears the box and then sends the query `pycon`\n",
    "\n",
    "It then hits return and checks to make sure that no results are returned before closing.\n",
    "\n",
    "Simple, right?\n",
    "\n",
    "Now let's try to search for `Obama` on CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Amazing! **But complicated**. We can also use the forward and back buttons for the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "driver.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "driver.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And you could print (and thus save the page source) or put it into beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "driver.page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But this won't work magic, if it's not in the source in your browser then it won't be in the source for selenium either.\n",
    "\n",
    "We can also find all/multiple elements with the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "headlines = driver.find_elements_by_class_name(\"cnn-search__result-headline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for hl in headlines:\n",
    "    print(hl.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The value of accessing inaccessible content\n",
    "\n",
    "This week we are examining at Malmgren RD, Ottino JM, Amaral LAN. (2010). The role of mentorship on protégé performance. Nature 463, 622-626.\n",
    "\n",
    "The article relied entirely on data from the [Math Genealogy Network](https://www.genealogy.math.ndsu.nodak.edu) and [MathSciNet](https://mathscinet.ams.org/mathscinet/) to construct lineages of mentors and individual productivity. This research is not possible without extracting and combining these two data sources. Put together it unlocks the possibility of examining an important and previously inaccessible question at scale. \n",
    "\n",
    "Importantly, both of these websites lack the resources to provide an API to download the data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
